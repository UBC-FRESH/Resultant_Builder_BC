{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af92206d-eb86-4ae8-81c9-b09d27d641b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Preparing Layers\n",
    "This code takes layers directly downloaded from the Data BC catalogue and prepares them for the creation of a database for landscape level planning. \n",
    "There are a few starting steps that cannot be automated as they are specific to each project.\n",
    "\n",
    "    1. Create a feature class of the area of interest (AOI) for the dataset. \n",
    "    2. Download data - users might need to use their AOI to work around max download amounts.\n",
    "Note: for those downloading from the DataBC site, and using an AOI to define the interest area they downloaded layers are not clipped to the AOI but rather are clipped to the smallest square (or rectangle that contains all points of the AOI). This means that if you have an AOI that is in different sections (i.e. a planning area that is not continuous) it will use a rectangle that covers all the points. This may require splitting the AOI into two sections. This case is not included in the code, but it can be adapted to accomidate this.\n",
    "\n",
    "    3. Create a file geodatabase named \"Data_Inputs.gdb\" in a file location that makes logical sense for the analysis. \n",
    "    4. Create a file geodatabase named \"Data_Prep.gdb\" in a file location that makes logical sense for the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d25934-e186-426b-82d1-68c7c16474d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User definitions:\n",
    "\n",
    "#arc envrironment\n",
    "#Set the file directory to a .gdb that hosts all the raw input data for analysis\n",
    "##Recomended that this .gdb is called Data_Inputs.gdb\n",
    "\n",
    "Data_Inputs = r\"E:\\Newmont Carbon Project\\Data\\Data BC\\00_Script_creation_Test\\Data_Inputs.gdb\"\n",
    "\n",
    "#File location to the file directory that host the.gdb where the cleaned up data files will go. \n",
    "##Recomended to call this .gdb Data_Prep.gdb\n",
    "\n",
    "Data_Prep = r\"E:\\Newmont Carbon Project\\Data\\Data BC\\00_Script_creation_Test\\Data_Prep.gdb\"\n",
    "\n",
    "#Define your AOI file\n",
    "AOI = \"GoldenBear_AOI\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "841e95c2-2a52-4d5a-9300-c3ce9d5aac76",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'arcpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22976\\1129314346.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0marcpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mStart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'arcpy'"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "import time\n",
    "\n",
    "Start = time.time()\n",
    "\n",
    "# Define the work environment and the datasets/feature classes to work with.\n",
    "# The workspace will be the file location of the Data_Inputs.gdb and needs to be defined by the user.\n",
    "# out_GDB is where the newly created and prepared layers will be created.\n",
    "\n",
    "arcpy.env.workspace = Data_Inputs\n",
    "out_GDB = Data_Prep\n",
    "\n",
    "# Feature classes definition\n",
    "# This section of code defines the differenet layers and provides them with a short name for easier referencing.\n",
    "# This code is set up to use the layer names of data directly downloaded from the DataBC catalogue. \n",
    "# Make sure to change the name of the AOI layer to match what you have called this feature class. \n",
    "# If you are missing some layers, simply comment out those lines here and in the following dictionary step.\n",
    "\n",
    "Agric = \"WHSE_LEGAL_ADMIN_BOUNDARIES_OATS_ALR_POLYS\"\n",
    "BEC = \"BEC\"\n",
    "Caribou = \"WHSE_WILDLIFE_INVENTORY_GCPB_CARIBOU_POPULATION_SP\"\n",
    "IR = \"WHSE_ADMIN_BOUNDARIES_CLAB_INDIAN_RESERVES\"\n",
    "FN_SOI = \"QSOI_BCREG_polygon\"\n",
    "Lakes = \"WHSE_LAND_AND_NATURAL_RESOURCE_EAUBC_LAKES_SP\"\n",
    "LU = \"WHSE_LAND_USE_PLANNING_RMP_LANDSCAPE_UNIT_SVW\"\n",
    "OGMA = \"WHSE_LAND_USE_PLANNING_RMP_OGMA_LEGAL_CURRENT_SVW\"\n",
    "Parks_Fed = \"WHSE_ADMIN_BOUNDARIES.CLAB_NATIONAL_PARKS\"\n",
    "Parks_Prv = \"WHSE_ADMIN_BOUNDARIES_ADM_BC_PARKS_REGIONS_SP\"\n",
    "Rec = \"WHSE_FOREST_VEGETATION_REC_VISUAL_LANDSCAPE_INVENTORY\"\n",
    "Results = \"WHSEFOREST_VEGETATION.VEG_CONSOLIDATED_CUT_BLOCKS_SP\"\n",
    "Rivers = \"WHSE_BASEMAPPING_FWA_RIVERS_POLY\"\n",
    "F_Roads = \"WHSE_FOREST_TENURE.FTEN_ROAD_LINES\"\n",
    "Roads = \"DRA_MPAR_line\"\n",
    "Streams = \"WHSE_BASEMAPPING_FWA_STREAM_NETWORKS_SP\"\n",
    "F_Own = \"F_OWN_polygon\"\n",
    "UWR = \"WHSE_WILDLIFE_MANAGEMENT_WCP_WILDLIFE_HABITAT_AREA_POLY\"\n",
    "VRI = \"GoldenBear_R1_Poly\"\n",
    "Watersheds = \"COM_WS_PUB_polygon\"\n",
    "Geo_watersheds = \"WHSE_FISH_WDIC_WATERSHED_GROUP_POLY\"\n",
    "Wetlands = \"WHSE_BASEMAPPING_FWA_WETLANDS_POLY\"\n",
    "WHA_a = \"WHSE_WILDLIFE_MANAGEMENT_WCP_WILDLIFE_HABITAT_AREA_POLY\"\n",
    "WHA_p = \"WHA_proposed\"\n",
    "\n",
    "# Dictionary of all feature classes; \n",
    "# dict key = variable defined above for each feature class name in the workspace GDB\n",
    "# Make sure that all of the layers you defined above are included here. \n",
    "# Comment out any layers that are missing. \n",
    "\n",
    "layersDict = {\n",
    "    aoi: \"AOI\",\n",
    "    Agric: \"Agric\",\n",
    "    BEC: \"BEC\",\n",
    "    Caribou: \"Caribou\",\n",
    "    IR: \"IR\",\n",
    "    FN_SOI: \"FN_SOI\",\n",
    "    Lakes: \"Lakes\",\n",
    "    LU: \"LU\",\n",
    "    OGMA: \"OGMA\",\n",
    "    Parks_Fed: \"Parks_Fed\",\n",
    "    Parks_Prv: \"Parks_Prv\",\n",
    "    Rec: \"Rec\",\n",
    "    Results: \"Results\",\n",
    "    Rivers: \"Rivers\",\n",
    "    F_Roads: \"F_Roads\"\n",
    "    Roads: \"Roads\",\n",
    "    Streams: \"Streams\",\n",
    "    F_Own: \"F_Own\",\n",
    "    UWR: \"UWR\",\n",
    "    VRI: \"VRI\",\n",
    "    Watersheds: \"Watersheds\",\n",
    "    Geo_watersheds: \"Geo_watersheds\",\n",
    "    Wetlands: \"Wetlands\",\n",
    "    WHA_a: \"WHA_a\",\n",
    "    WHA_p: \"WHA_p\"\n",
    "}\n",
    "\n",
    "# Project feature classes to a common coordinate system defined by VRI\n",
    "# This section of code ensure that each layer is using the same projection and coordinate system.\n",
    "# It uses the VRI as reference, and would work with any similar dataset.\n",
    "# It creates a new layer for each of the defined layers called DEFINEDNAME_proj. Here _proj stands for projection.\n",
    "# These layers will be created in the Data_Inputs.gdb\n",
    "# There will be no layer created called VRI_proj, since we are using the VRI as the reference data. We will account for this in the following step. \n",
    "\n",
    "desc = arcpy.Describe(VRI)\n",
    "sr = desc.spatialReference\n",
    "\n",
    "for m in layersDict.keys():\n",
    "    if layersDict[m] != \"VRI\":\n",
    "        arcpy.Project_management(m, layersDict[m]+\"_proj\", sr)\n",
    "\n",
    "print(\"Data projection finished. Clipping to AOI...\")\n",
    "\n",
    "# Clip all feature classes to AOI_proj\n",
    "# This section clips all the layers to AOI_proj and exports them to the Data_Prep.gdb.\n",
    "\n",
    "#This section works with all layers excluding the aoi and VRI layers\n",
    "for m in layersDict.keys():\n",
    "    if layersDict[m] not in [aoi, \"VRI\"]:\n",
    "        arcpy.Clip_analysis(layersDict[m] + \"_proj\", \"AOI_proj\", out_GDB + \"\\\\\" + layersDict[m] + \"_proj_clip\")\n",
    "\n",
    "#This section clips the VRI and accounts for its different name in the step above.\n",
    "arcpy.Clip_analysis(VRI, \"AOI_proj\", out_GDB + \"\\\\\" + \"VRI\" + \"_proj_clip\")\n",
    "\n",
    "print(\"Clipping to AOI_proj finished. Repairing geometry...\")\n",
    "\n",
    "# Repair Geometry\n",
    "# This section fixes any geometry errors. This tool does not create a new feature class.\n",
    "# Instead it repairs any self intersections, invalid records. ect. \n",
    "\n",
    "arcpy.env.workspace = out_GDB\n",
    "for m in layersDict.keys():\n",
    "     arcpy.RepairGeometry_management(layersDict[m]+\"_proj_clip\")\n",
    "\n",
    "print('It took', round((time.time() - Start) / 60, 1), \"minutes to prepare the layers.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ef09f-803c-4edb-9f95-2397b63d155a",
   "metadata": {},
   "source": [
    "# Buffer Features\n",
    "The next section of code creates a buffer around rivers, streams, wetlands, lakes and roads. \n",
    "\n",
    "If there are other layers that you would like to buffer simply add in some extra lines of code and define the layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f621a4-d1c3-4c30-bee2-40e307916357",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this script buffers\n",
    "#rivers at 100m\n",
    "#streams at 20m\n",
    "#wetlands at 10m\n",
    "#lakes at 30m\n",
    "#roads right-of-way at 10m\n",
    "\n",
    "Start = time.time()\n",
    "\n",
    "#define work environment and datsets/fetaure classes to work with.\n",
    "arcpy.env.workspace = Data_Prep\n",
    "\n",
    "# Feature classes definition.\n",
    "lakes=\"Lakes_proj_clip\"\n",
    "rivers=\"Rivers_proj_clip\"\n",
    "F_roads = \"F_Roads_proj_clip\"\n",
    "roads=\"Roads_proj_clip\"\n",
    "streams=\"Streams_proj_clip\"\n",
    "wetlands=\"Wetlands_proj_clip\"\n",
    "\n",
    "#buffering\n",
    "arcpy.Buffer_analysis(lakes, lakes+\"_buff\", \"30 meters\", \"OUTSIDE_ONLY\", \"\", \"ALL\", \"\")\n",
    "print (\"finished Buffering lakes. buffering rivers...\")\n",
    "\n",
    "arcpy.Buffer_analysis(rivers, rivers+\"_buff\", \"100 meters\", \"OUTSIDE_ONLY\", \"\", \"ALL\", \"\")\n",
    "print (\"finished Buffering rivers. buffering roads...\")\n",
    "\n",
    "arcpy.Buffer_analysis(F_roads, F_roads+\"_buff\", \"10 meters\", \"FULL\", \"\", \"ALL\", \"\")\n",
    "print (\"finished Buffering forest roads. buffering streams...\")\n",
    "\n",
    "arcpy.Buffer_analysis(roads, roads+\"_buff\", \"10 meters\", \"FULL\", \"\", \"ALL\", \"\")\n",
    "print (\"finished Buffering roads. buffering streams...\")\n",
    "\n",
    "arcpy.Buffer_analysis(streams, streams+\"_buff\", \"20 meters\", \"FULL\", \"\", \"ALL\", \"\")\n",
    "print (\"finished Buffering streams. buffering wetlands...\")\n",
    "\n",
    "arcpy.Buffer_analysis(wetlands, wetlands+\"_buff\", \"10 meters\", \"OUTSIDE_ONLY\", \"\", \"ALL\", \"\")\n",
    "print (\"finished Buffering wetlands...\")\n",
    "\n",
    "print('It took', round((time.time() - Start) / 60, 1), \"minutes to buffer the features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c7c6b-b596-4f57-a8a6-24496f6bb548",
   "metadata": {},
   "source": [
    "After running the above section of code the user must manually fix the topology errors that the different layers may contain. \n",
    "The most common topology error is when there is an overlap between two polygons or a small gap. For landscape level planning purposes we cannot have either of these and they must be correct. \n",
    "\n",
    "Follow the steps provided:\n",
    "\n",
    "    1. Right click on the Data_Prep.gdb in the catalog window.\n",
    "    2. Select \"New\" then \"Feature Dataset\". \n",
    "    3. Name the new feature dataset \"fds\". \n",
    "    4. Import the correct coordinate system for the first feature class.\n",
    "    5. Leave all other options as the default.\n",
    "    6. Drag the first feature class into the new feature dataset (fds).\n",
    "    7. Right click on fds and select \"new\" then \"create new topology\".\n",
    "    8. Define a xy cluster tolerance rule of 0.1m\n",
    "    9. Select the layer you dragged in and click next.\n",
    "    10. Select the layer, and set the rules to \"Must Not Overlap\". \n",
    "    11. Add an additional rule for that layer of \"Must Not Have Gaps\".\n",
    "    12. Click next.\n",
    "    13. Double check options and click \"Finish\"\n",
    "    14. Once validated, right click on the topology class created in fds. Select \"properties\".\n",
    "    15. Navigate to \"Errors\" and click \"Generate Summary\".\n",
    "    16. The report will show how many errors. \n",
    "    17. If none then the topology class created for the first feature class can be deleted.\n",
    "    18. The feature class can be dragged back into the Data_Prep.gdb.\n",
    "    19. The next layer can be dragged into fds.\n",
    "    20. Complete steps for all feature classes\n",
    "\n",
    "To repair overlap errors:\n",
    "\n",
    "    1. Drag the topology feature that has errors into the Map. \n",
    "    2. In the Map window erros are topology error are shown in red. \n",
    "    3. Add the \"Topology\" toolbar.\n",
    "    4. Right click the feature class from the table of contents. Select \"Edit Features\" then \"Start Editing\".\n",
    "    5. Add the \"Editor\" toolbar.\n",
    "    6. From the Topology toolbar open the error inspector.\n",
    "    7. Search for overlap errors.\n",
    "    8. Select the first error, right click, and select \"Merge\".\n",
    "    9. Repair Gap errors, but right clicking a gap error in the \"Error Inspector\" window.\n",
    "    10. Select \"Zoom To\" then \"Create Feature\".\n",
    "    11. Select the new feature and an adjacent polygon. From the \"Editor\" pull-down in the \"Editor\" toolbar select merge. \n",
    "    12. Fix all topology erros.\n",
    "    13. When all are corrected soom to the entire feature class. \n",
    "    14. In the \"topology\" toolbar select \"Validate Topology in Current Extent\".\n",
    "    15. Examine the \"Error Inspector\" window for other errors.\n",
    "    16. Repeat until all errors are repaired.\n",
    "    17. In the \"Editor\" menu, select \"Stop Editing\" and \"Yes\" to save the edits. \n",
    "    18. You can now removed the topology and feature class layers from the Map.\n",
    "    19. The topology class can be deleted and the feature class dragged into the Data_Prep.gdb.\n",
    "    \n",
    "It is possible to check for Topology errors in multiple layers at once (if they share a projection system), but sometimes it messes up. It is best to work one layer at a time until all errors are fixed.\n",
    "\n",
    "With data from DataBC there should be few to no erros, but you still need to check. \n",
    "\n",
    "Sometimes when you move the feature class out of fds you might get the error \"Move Failed. Cannot aquire a schema lock becuause of an existing lock\". Just refesh fds and try again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3b0a0d-3754-4e19-90d0-af372d6b2d6c",
   "metadata": {},
   "source": [
    "# Attribute Clean Up\n",
    "In the previous sets new feature classes were built that are clipped to the AOI, were buffered and had topology erros fixed. This next step removes the columns in the feature classes attribute tables that are not needed for analysis. \n",
    "\n",
    "The goal is to only include the fields that are required. \n",
    "\n",
    "In this step we will also check to see if a feature class has any features. It isn't uncommon that after collecting the data and clipping to size that there might not be any features. For example, there may not be any OGMA's that overlap with your AOI. These layers can be removed from this and future steps.\n",
    "\n",
    "The following code uses the Delete field geoprocessing tool to remove extra fields. \n",
    "This code was developed for the data avaliable through the Data_BC Catalog\n",
    "Remember that you don't need to worry about the features that were buffered (all fields were removed during the buffering step).\n",
    "\n",
    "It is highly recommended that you check each features attribute table after this step. Some feaures have two rows named OBJECTID, one that is removable and one that is not. Delete one manually if possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcaa715-4dd0-4b21-8b06-7617be90ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import time\n",
    "\n",
    "Start = time.time()\n",
    "\n",
    "# Define the work environment and the datasets/feature classes to work with. \n",
    "arcpy.env.workspace = r\"E:\\Newmont Carbon Project\\Data\\Data BC\\00_Script_creation_Test\\Data_Prep.gdb\"\n",
    "\n",
    "# Feature classes definition\n",
    "aoi=\"AOI_proj_clip\"\n",
    "Agric = \"Agric_proj_clip\"\n",
    "BEC = \"BEC_proj_clip\"\n",
    "Caribou = \"Caribou_proj_clip\"\n",
    "IR = \"IR_proj_clip\"\n",
    "FN_SOI = \"FN_SOI_proj_clip\"\n",
    "LU = \"LU_proj_clip\"\n",
    "OGMA = \"OGMA_proj_clip\"\n",
    "Parks_Fed = \"Parks_Fed_proj_clip\"\n",
    "Parks_Prv = \"Parks_Prv_proj_clip\"\n",
    "Rec = \"Rec_proj_clip\"\n",
    "F_own = \"F_Own_proj_clip\"\n",
    "UWR = \"UWR_proj_clip\"\n",
    "Watersheds = \"Watersheds_proj_clip\"\n",
    "Geo_watersheds = \"Geo_watersheds_proj_clip\"\n",
    "WHA_a = \"WHA_a_proj_clip\"\n",
    "WHA_p = \"WHA_p_proj_clip\"\n",
    "\n",
    "# Delete fields from the Agric\n",
    "print(\"Deleting fields from Agric...\")\n",
    "\n",
    "# Get a list of all fields in the feature class\n",
    "fields = [field.name for field in arcpy.ListFields(Agric)]\n",
    "\n",
    "# Determine the fields to keep (containing \"SHAPE\" and \"OBJECTID\")\n",
    "fields_to_keep = [field for field in fields if \"SHAPE\" in field or \"OBJECTID\" in field]\n",
    "\n",
    "# Determine the fields to delete\n",
    "fields_to_delete = [field for field in fields if field not in fields_to_keep]\n",
    "\n",
    "# Delete the fields from the feature class\n",
    "arcpy.DeleteField_management(Agric, fields_to_delete)\n",
    "\n",
    "# Delete fields from the AOI\n",
    "print(\"Deleting fields from aoi...\")\n",
    "fields = [field.name for field in arcpy.ListFields(aoi)]\n",
    "fields_to_keep = [field for field in fields if \"Shape\" in field or \"OBJECTID\" in field]\n",
    "fields_to_delete = [field for field in fields if field not in fields_to_keep]\n",
    "arcpy.DeleteField_management(aoi, fields_to_delete)\n",
    "\n",
    "# Delete fields from the BEC\n",
    "print(\"Deleting fields from BEC...\")\n",
    "fields = [field.name for field in arcpy.ListFields(BEC)]\n",
    "fields_to_keep = [field for field in fields if \"Shape\" in field or \"OBJECTID\" in field or \"NATURAL_DISTURBANCE\" in field]\n",
    "fields_to_delete = [field for field in fields if field not in fields_to_keep]\n",
    "arcpy.DeleteField_management(BEC, fields_to_delete)\n",
    "\n",
    "# Delete fields from the Caribou\n",
    "print(\"Deleting fields from Caribou...\")\n",
    "target = Caribou\n",
    "fields = [field.name for field in arcpy.ListFields(target)]\n",
    "fields_to_keep = [field for field in fields if \"SHAPE\" in field or \"OBJECTID*\" in field or \"HERD_STATUS\" in field or \"HERD_NAME\" in field or \"RISK_STATUS\" in field]\n",
    "fields_to_delete = [field for field in fields if field not in fields_to_keep]\n",
    "arcpy.DeleteField_management(target, fields_to_delete)\n",
    "\n",
    "# Delete fields from the F_Own\n",
    "print(\"Deleting fields from F_Own...\")\n",
    "target = F_Own\n",
    "fields = [field.name for field in arcpy.ListFields(target)]\n",
    "fields_to_keep = [field for field in fields if \"Shape\" in field or \"OBJECTID_1\" in field or \"SCHEDULE\" in field or \"OWN_DESC\" in field]\n",
    "fields_to_delete = [field for field in fields if field not in fields_to_keep]\n",
    "arcpy.DeleteField_management(target, fields_to_delete)\n",
    "\n",
    "# Delete fields from the FN_SOI\n",
    "print(\"Deleting fields from FN_SOI...\")\n",
    "target = FN_SOI\n",
    "fields = [field.name for field in arcpy.ListFields(target)]\n",
    "fields_to_keep = [field for field in fields if \"Shape\" in field or \"OBJECTID_1\" in field or \"NAME\" in field]\n",
    "fields_to_delete = [field for field in fields if field not in fields_to_keep]\n",
    "arcpy.DeleteField_management(target, fields_to_delete)\n",
    "\n",
    "# Delete fields from the Geo_Watershed\n",
    "print(\"Deleting fields from Geo_watersheds...\")\n",
    "target = Geo_watersheds\n",
    "fields = [field.name for field in arcpy.ListFields(target)]\n",
    "fields_to_keep = [field for field in fields if \"SHAPE\" in field or \"OBJECTID*\" in field or \"WATERSHED_GROUP_NAME\" in field]\n",
    "fields_to_delete = [field for field in fields if field not in fields_to_keep]\n",
    "arcpy.DeleteField_management(target, fields_to_delete)\n",
    "\n",
    "# Delete fields from the IR\n",
    "print(\"Deleting fields from IR...\")\n",
    "target = IR\n",
    "fields = [field.name for field in arcpy.ListFields(target)]\n",
    "fields_to_keep = [field for field in fields if \"SHAPE\" in field or \"OBJECTID*\" in field or \"ENGLISH_NAME\" in field]\n",
    "fields_to_delete = [field for field in fields if field not in fields_to_keep]\n",
    "arcpy.DeleteField_management(target, fields_to_delete)\n",
    "\n",
    "# Delete fields from the LU\n",
    "print(\"Deleting fields from LU...\")\n",
    "target = LU\n",
    "fields = [field.name for field in arcpy.ListFields(target)]\n",
    "fields_to_keep = [field for field in fields if \"SHAPE\" in field or \"OBJECTID*\" in field or \"LANDSCAPE_UNIT_NAME\" in field  or \"BIODIVERSITY_EMPHASIS_OPTION\" in field]\n",
    "fields_to_delete = [field for field in fields if field not in fields_to_keep]\n",
    "arcpy.DeleteField_management(target, fields_to_delete)\n",
    "\n",
    "# Delete fields from the OGMA\n",
    "print(\"Deleting fields from OGMA...\")\n",
    "target = OGMA\n",
    "fields = [field.name for field in arcpy.ListFields(target)]\n",
    "fields_to_keep = [field for field in fields if \"SHAPE\" in field or \"OBJECTID*\" in field]\n",
    "fields_to_delete = [field for field in fields if field not in fields_to_keep]\n",
    "arcpy.DeleteField_management(target, fields_to_delete)\n",
    "\n",
    "# Delete fields from the Parks_Prv\n",
    "print(\"Deleting fields from Parks_Prv...\")\n",
    "target = Parks_Prv\n",
    "fields = [field.name for field in arcpy.ListFields(target)]\n",
    "fields_to_keep = [field for field in fields if \"SHAPE\" in field or \"OBJECTID*\" in field or \"PROTECTED_LANDS_NAME\" in field]\n",
    "fields_to_delete = [field for field in fields if field not in fields_to_keep]\n",
    "arcpy.DeleteField_management(target, fields_to_delete)\n",
    "\n",
    "# Delete fields from the Parks_Fed\n",
    "print(\"Deleting fields from Parks_Fed...\")\n",
    "target = Parks_Fed\n",
    "fields = [field.name for field in arcpy.ListFields(target)]\n",
    "fields_to_keep = [field for field in fields if \"SHAPE\" in field or \"OBJECTID*\" in field or \"ENGLISH_NAME\" in field]\n",
    "fields_to_delete = [field for field in fields if field not in fields_to_keep]\n",
    "arcpy.DeleteField_management(target, fields_to_delete)\n",
    "\n",
    "# Delete fields from the Rec\n",
    "print(\"Deleting fields from Rec...\")\n",
    "target = Rec\n",
    "fields = [field.name for field in arcpy.ListFields(target)]\n",
    "fields_to_keep = [field for field in fields if \"SHAPE\" in field or \"OBJECTID*\" in field or \"REC_VAC_FINAL_VALUE_CODE\" in field or \"REC_EVQO_CODE\" in field]\n",
    "fields_to_delete = [field for field in fields if field not in fields_to_keep]\n",
    "arcpy.DeleteField_management(target, fields_to_delete)\n",
    "\n",
    "# Delete fields from the UWR\n",
    "print(\"Deleting fields from UWR...\")\n",
    "target = UWR\n",
    "fields = [field.name for field in arcpy.ListFields(target)]\n",
    "fields_to_keep = [field for field in fields if \"SHAPE\" in field or \"OBJECTID*\" in field or \"UWR_UNIT_NUMBER\" in field or \"UWR_NUMBER\" in field  or \"TIMBER_HARVEST_CODE\" in field]\n",
    "fields_to_delete = [field for field in fields if field not in fields_to_keep]\n",
    "arcpy.DeleteField_management(target, fields_to_delete)\n",
    "\n",
    "# Delete fields from the Watersheds\n",
    "print(\"Deleting fields from Watersheds...\")\n",
    "target = Watersheds\n",
    "fields = [field.name for field in arcpy.ListFields(target)]\n",
    "fields_to_keep = [field for field in fields if \"Shape\" in field or \"OBJECTID_1\" in field or \"COMMUNITY_WS_NAME\" in field]\n",
    "fields_to_delete = [field for field in fields if field not in fields_to_keep]\n",
    "arcpy.DeleteField_management(target, fields_to_delete)\n",
    "\n",
    "# Delete fields from the WHA_a\n",
    "print(\"Deleting fields from WHA_a...\")\n",
    "target = WHA_a\n",
    "fields = [field.name for field in arcpy.ListFields(target)]\n",
    "fields_to_keep = [field for field in fields if \"SHAPE\" in field or \"OBJECTID*\" in field or \"COMMON_SPECIES_NAME\" in field or \"TIMBER_HARVEST_CODE\" in field]\n",
    "fields_to_delete = [field for field in fields if field not in fields_to_keep]\n",
    "arcpy.DeleteField_management(target, fields_to_delete)\n",
    "\n",
    "# Delete fields from the WHA_p\n",
    "print(\"Deleting fields from WHA_p...\")\n",
    "target = WHA_p\n",
    "fields = [field.name for field in arcpy.ListFields(target)]\n",
    "fields_to_keep = [field for field in fields if \"SHAPE\" in field or \"OBJECTID*\" in field or \"COMMON_SPECIES_NAME\" in field or \"TIMBER_HARVEST_CODE\" in field]\n",
    "fields_to_delete = [field for field in fields if field not in fields_to_keep]\n",
    "arcpy.DeleteField_management(target, fields_to_delete)\n",
    "\n",
    "print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515df690-9393-42af-986d-432b2bd6b31f",
   "metadata": {},
   "source": [
    "# Depletions\n",
    "This step is for identifying areas that have been recently harvested but are not yet accounted for in the VRI. Sometimes the Results (Consolidated cutblocks) and the VRI has overlapping data. The first step to ensure you do not double count areas is to simplifiy the polygon shapes is to erase the polygones with valid (non-null) harvest dates in the VRI from the Results feature class. \n",
    "\n",
    "Then in \"Editor: mode, manually edit the remaining polygons in Results. \n",
    "    \n",
    "    Delete small isolated polygons.\n",
    "    Disolve the Results by disturbance date and check and repair overlap topography errors.\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687a349-ccc8-4803-8860-ed77fc2a4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import time\n",
    "\n",
    "Start = time.time()\n",
    "\n",
    "#define work environment and datasets/feature classes to work with\n",
    "arcpy.env.workspace = r\"E:\\Newmont Carbon Project\\Data\\Data BC\\00_Script_creation_Test\\Data_Prep.gdb\"\n",
    "\n",
    "Results = \"Results_proj_clip\"\n",
    "VRI = \"VRI_proj_clip\"\n",
    "\n",
    "#add fields to the Results_Opening\n",
    "try:\n",
    "    arcpy.AddField_management(Results, \"Dist_Year\", \"LONG\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#Update Dist_Year\n",
    "with arcpy.da.UpdateCursor(Results, [\"DISTURBANCE_END_DATE\", \"Dist_Year\", \"OPENING_WHEN_CREATED\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        row[1] = None\n",
    "        if row[0] is not None:\n",
    "            row[1] = int(str(row[0])[:4])\n",
    "        else:\n",
    "            row[1] = int(str(row[2])[:4])\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print('Finished updating Dist_Year field. Erase VRI harvested polygons...')\n",
    "\n",
    "#Erases VRI\n",
    "arcpy.MakeFeatureLayer_management(VRI, \"VRI_harv\", '\"HARVEST_DATE\" is not null')\n",
    "arcpy.Erase_analysis(Results, \"VRI_harv\", \"Results_erase\")\n",
    "print('Finished erasing VRI harvested polygons. Dissolving by Dist_Year...')\n",
    "\n",
    "# dissolve\n",
    "arcpy.Dissolve_management(\"Results_erase\", \"Results_dissolved\", [\"Dist_Year\"], \"\", \"SINGLE_PART\", \"\")\n",
    "\n",
    "print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef77daa7-6f5e-425d-81fc-e8efe7d960d4",
   "metadata": {},
   "source": [
    "A final visual check should be done.\n",
    "Add a basemap. \n",
    "Visualize the VRI - Right click Properties, then Definition Query. Add in the Definition Query field \"HARVEST_DATE is not null\".\n",
    "Add Results to the map viewer.\n",
    "Format both layers so that they visualize as hollow with visible boundaries. \n",
    "At this point it should be possible to see if some harvested areas are missing information (aka. not listed with being harvested). They will appear as cutblocks on the basemap but will not have an outline from either the VRI or Results file. \n",
    "Any missing areas will need to have digitized as new polygons.\n",
    "This is done in editor mode (add the new polygon to the Results layer). \n",
    "You will need to make an educated guess when you think the disturbance occured. This can be done by comparing it to similar looking cutblocks in the area. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b8daf-13ea-4649-ae7c-35d09470042b",
   "metadata": {},
   "source": [
    "# Build Resultant File\n",
    "Builds the resultant files\n",
    "This section of code takes the cleaned up layers and attirbues tables and pushes them together into one layer called the resultant. \n",
    "It then tidies up the resulant file by reparing the geometry, changing multipart to single part polygons and eliminating sliver and low area polygons by merging them with their largest neighbour. \n",
    "This code builds on and updates original code y Dr. Cosmin Mann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7be12f-0138-4e1c-9ac2-d8caeacf1216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import time\n",
    "Start = time.time()\n",
    "\n",
    "#eliminator function to eliminate the sliver polygons \n",
    "def eliminator(fc,fcout):\n",
    "    arcpy.MakeFeatureLayer_management(fc, \"tempLayer\")\n",
    "    arcpy.SelectLayerByAttribute_management(\"tempLayer\", \"NEW_SELECTION\", '\"Shape_Area\" < 1000')\n",
    "    arcpy.Eliminate_management(\"tempLayer\", fcout)\n",
    "    arcpy.Delete_management(\"tempLayer\")\n",
    "\n",
    "arcpy.env.workspace = r\"E:\\Newmont Carbon Project\\Data\\Data BC\\00_Script_creation_Test\\Data_Prep.gdb\"\n",
    "\n",
    "arcpy.Identity_analysis (\"AOI_proj_clip\", \"IR_proj_clip\", \"temp1\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp1\", \"Agric_proj_clip\", \"temp2\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp2\", \"BEC_proj_clip\", \"temp3\",\"ALL\", 0.1)\n",
    "\n",
    "arcpy.Identity_analysis (\"temp3\", \"Caribou_proj_clip\", \"temp4\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp4\", \"F_Own_proj_clip\", \"temp5\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp5\", \"FN_SOI_proj_clip\", \"temp6\",\"ALL\", 0.1)\n",
    "\n",
    "arcpy.Identity_analysis (\"temp6\", \"Geo_watersheds_proj_clip\", \"temp7\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp7\", \"LU_proj_clip\", \"temp8\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp8\", \"OGMA_proj_clip\", \"temp9\",\"ALL\", 0.1)\n",
    "\n",
    "arcpy.Identity_analysis (\"temp9\", \"Parks_Fed_proj_clip\", \"temp10\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp10\", \"Parks_Prv_proj_clip\", \"temp11\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp11\", \"Rec_proj_clip\", \"temp12\",\"ALL\", 0.1)\n",
    "\n",
    "arcpy.Identity_analysis (\"temp12\", \"Results\", \"temp13\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp13\", \"UWR_proj_clip\", \"temp14\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp14\", \"Watersheds_proj_clip\", \"temp15\",\"ALL\", 0.1)\n",
    "\n",
    "arcpy.Identity_analysis (\"temp15\", \"WHA_a_proj_clip\", \"temp16\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp16\", \"WHA_p_proj_clip\", \"temp17\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp17\", \"Lakes_proj_clip\", \"temp18\",\"ALL\", 0.1)\n",
    "\n",
    "arcpy.Identity_analysis (\"temp18\", \"Rivers_proj_clip\", \"temp19\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp19\", \"wetlands_proj_clip\", \"temp20\",\"ALL\", 0.1)\n",
    "\n",
    "eliminator(\"temp20\",\"temp20e\")\n",
    "print(\"administrative_management completed: identifying inventory\")\n",
    "\n",
    "arcpy.Identity_analysis (\"temp20e\", \"VRI_proj_clip\", \"temp21\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp21\", \"Lakes_proj_clip_buff\", \"temp22\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp22\", \"Rivers_proj_clip_buff\", \"temp23\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp23\", \"Roads_proj_clip_buff\", \"temp24\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp24\", \"F_Roads_proj_clip_buff\", \"temp25\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp25\", \"Streams_proj_clip_buff\", \"temp26\",\"ALL\", 0.1)\n",
    "arcpy.Identity_analysis (\"temp26\", \"Wetlands_proj_clip_buff\", \"temp27\",\"ALL\", 0.1)\n",
    "\n",
    "print (\"temp27 is the final feature dataset\")\n",
    "print (\"repairing Geom, multi to sgl, eliminate more polys...\")\n",
    "\n",
    "arcpy.RepairGeometry_management (\"temp27\")\n",
    "arcpy.MultipartToSinglepart_management(\"temp27\",\"temp27_sgl\")\n",
    "eliminator(\"temp27_sgl\",\"temp27_sgl_e1\")\n",
    "eliminator(\"temp27_sgl_e1\",\"temp27_sgl_e2\")\n",
    "print ('It took ', round((time.time()-Start)/60,1), \" minutes to run this script.\")\n",
    "\n",
    "## Manually double check the file, you may need to run the elimate tool again if there are still tiny polygons.\n",
    "## The final manual steps are to delete the Temporary files that were created and to delete the FID_Temp... attributes that were created. These columns are not needed and add usless data. \n",
    "## Finally rename the data set and put into the resultant geodatabase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b3739c-f8d5-4b9c-8608-63a513798113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "867ef478-2b5b-4a97-9526-45f581dd8606",
   "metadata": {},
   "source": [
    "# Netdown Creation\n",
    "This section of code delinates different areas of the land base. It creates three different column and sorts the data based on different values in the attribute tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fed066-fb11-4e0e-a965-3eff1b168150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import time\n",
    "Start = time.time()\n",
    "\n",
    "arcpy.env.workspace = r\"E:\\Newmont Carbon Project\\Data\\Data BC\\00_Script_creation_Test\\Resultant_v1.gdb\"\n",
    "\n",
    "fc = \"Resultant_v1\" \n",
    "try:\n",
    "    arcpy.AddField_management(fc, \"contclass\", \"TEXT\", \"\", \"\", 1) \n",
    "except:\n",
    "    pass \n",
    "try:\n",
    "    arcpy.AddField_management(fc, \"rollup\", \"TEXT\", \"\", \"\", 50) \n",
    "except:\n",
    "    pass \n",
    "try:\n",
    "    arcpy.AddField_management(fc, \"netdown\", \"TEXT\", \"\", \"\", 50) \n",
    "except:\n",
    "    pass \n",
    "try:\n",
    "    arcpy.AddField_management(fc, \"THLB_Area\", \"Double\") \n",
    "except:\n",
    "    pass\n",
    "\n",
    "#dictionary that enables the use of fields name in the update cursor \n",
    "flist = arcpy.ListFields(fc)\n",
    "fdic = {} \n",
    "fl = []\n",
    "for f in flist:\n",
    "    fdic[f.name] = flist.index(f) \n",
    "    fl.append(f.name)\n",
    "\n",
    "with arcpy.da.UpdateCursor(fc, fl) as cursor:\n",
    "    for row in cursor:\n",
    "        row[fdic[\"contclass\"]] = \"C\" \n",
    "        row[fdic[\"netdown\"]] = \"THLB\" \n",
    "        row[fdic[\"rollup\"]] = \"THLB\"\n",
    "        row[fdic[\"THLB_Area\"]] = row[fdic[\"Shape_Area\"]]/10000\n",
    "\n",
    "       ## Riparian from lake, wetland, river, and Stream buffers \n",
    "        if row[fdic[\"FID_Lakes_proj_clip_buff\"]] > 0:\n",
    "            row[fdic[\"contclass\"]] = \"N\" \n",
    "            row[fdic[\"THLB_Area\"]] = 0 \n",
    "            row[fdic[\"netdown\"]] = \"6_04_Lake_Buffer\" \n",
    "            row[fdic[\"rollup\"]] = \"6_Riparian\"\n",
    "            \n",
    "        elif row[fdic[\"FID_Wetlands_proj_clip_buff\"]] > 0:\n",
    "            row[fdic[\"contclass\"]] = \"N\"\n",
    "            row[fdic[\"THLB_Area\"]] = 0\n",
    "            row[fdic[\"netdown\"]] = \"6_03_Wetland_Buffer\"\n",
    "            row[fdic[\"rollup\"]] = \"6_Riparian\"\n",
    "\n",
    "        elif row[fdic[\"FID_Rivers_proj_clip_buff\"]] > 0:\n",
    "            row[fdic[\"contclass\"]] = \"N\"\n",
    "            row[fdic[\"THLB_Area\"]] = 0\n",
    "            row[fdic[\"netdown\"]] = \"6_02_River_Buffer\"\n",
    "            row[fdic[\"rollup\"]] = \"6_Riparian\"\n",
    "\n",
    "        elif row[fdic[\"FID_Streams_proj_clip_buff\"]] > 0:\n",
    "            row[fdic[\"contclass\"]] = \"N\"\n",
    "            row[fdic[\"THLB_Area\"]] = 0\n",
    "            row[fdic[\"netdown\"]] = \"6_01_Stream_Buffer\"\n",
    "            row[fdic[\"rollup\"]] = \"6_Riparian\"\n",
    "            \n",
    "        ## Non-productive/non-commercial stands\n",
    "        elif row[fdic[\"SPECIES_CD_1\"]] not in [None, \"\"]:\n",
    "            if row[fdic[\"NON_FOREST_DESCRIPTOR\"]] not in [None, \"\"]:\n",
    "                row[fdic[\"contclass\"]] = \"N\"\n",
    "                row[fdic[\"THLB_Area\"]] = 0\n",
    "                row[fdic[\"netdown\"]] = (\"5_03_Non_Prod_\" + str(row[fdic[\"NON_FOREST_DESCRIPTOR\"]]))[0:50]\n",
    "                row[fdic[\"rollup\"]] = \"5_Non_Productive\"        \n",
    "     \n",
    "        ##Environmentally Sensitive Areas (ESA)\n",
    "        elif row[fdic[\"MODIFYING_PROCESS\"]] not in [None, \"\", \"N\"]:\n",
    "            row[fdic[\"contclass\"]] = \"N\"\n",
    "            row[fdic[\"THLB_Area\"]] = 0\n",
    "            row[fdic[\"netdown\"]] = (\"5_01_Sensitive_\" + str(row[fdic[\"MODIFYING_PROCESS\"]]))[0:50]\n",
    "            row[fdic[\"rollup\"]] = \"5_Non_Productive\"\n",
    "\n",
    "        elif row[fdic[\"HARVEST_DATE\"]] is None and row[fdic[\"SITE_INDEX\"]] is not None and row[fdic[\"SITE_INDEX\"]] < 8:\n",
    "        ## and row[fdic[\"FID_Results\"]]<1 \n",
    "            row[fdic[\"contclass\"]] = \"N\"\n",
    "            row[fdic[\"THLB_Area\"]] = 0 \n",
    "            row[fdic[\"netdown\"]] = \"4_01_Poor_sites\" \n",
    "            row[fdic[\"rollup\"]] = \"4_Non_Commercial\"\n",
    "            \n",
    "        ## Reserves+Habitat\n",
    "        # current data set doesn't have no harvest all is conditional harvest\n",
    "        #if row[fdic[\"FID_UWR_proj_clip\"]] >0 and row[fdic[\"TIMBER_HARVEST_CODE\"]]==\"NO HARVEST ZONE\":\n",
    "         #   row[fdic[\"contclass\"]] = \"N\"\n",
    "          #  row[fdic[\"THLB_Area\"]] = 0\n",
    "           # row[fdic[\"netdown\"]] = (\"3_04_HabitatUWR_\"+ str(row[fdic[\"UWR_NUMBER\"]]))[0:50] \n",
    "            #row[fdic[\"rollup\"]] = \"3_Reserves\"\n",
    "\n",
    "        elif row[fdic[\"FID_WHA_a_proj_clip\"]] is not None and row[fdic[\"TIMBER_HARVEST_CODE_1\"]]==\"NO HARVEST ZONE\": \n",
    "            row[fdic[\"contclass\"]] = \"N\"\n",
    "            row[fdic[\"THLB_Area\"]] = 0\n",
    "            row[fdic[\"netdown\"]] = (\"3_03_HabitatWHa_\"+ str(row[fdic[\"COMMON_SPECIES_NAME\"]]))[0:50] \n",
    "            row[fdic[\"rollup\"]] = \"3_Reserves\"\n",
    "\n",
    "        #if row[fdic[\"FID_WHA_p_proj_clip\"]] >0 and row[fdic[\"TIMBER_HARVEST_CODE_12\"]]==\"NO HARVEST ZONE\": \n",
    "        #    row[fdic[\"contclass\"]] = \"N\"\n",
    "        #    row[fdic[\"THLB_Area\"]] = 0\n",
    "        #    row[fdic[\"netdown\"]] = (\"3_03_HabitatWHp_\"+ str(row[fdic[\"COMMON_SPECIES_NAME_1\"]]))[0:50] \n",
    "        #    row[fdic[\"rollup\"]] = \"3_Reserves\"\n",
    "\n",
    "        #if row[fdic[\"FID_OGMA_proj_clip\"]] >0: \n",
    "        #    row[fdic[\"contclass\"]] = \"N\" \n",
    "        #    row[fdic[\"THLB_Area\"]] = 0 \n",
    "        #    row[fdic[\"netdown\"]] = (\"3_02_OGMA_\")[0:50] \n",
    "        #    row[fdic[\"rollup\"]] = \"3_Reserves\"\n",
    "        \n",
    "        ##Non Forest \n",
    "        #VRI Non_Forest\n",
    "\n",
    "        elif row[fdic[\"BCLCS_LEVEL_1\"]] in [None, \"\", \"U\"]: \n",
    "            row[fdic[\"contclass\"]] = \"X\" \n",
    "            row[fdic[\"THLB_Area\"]] = 0 \n",
    "            row[fdic[\"netdown\"]] = \"2_08_Not_Typed\" \n",
    "            row[fdic[\"rollup\"]] = \"2_Non_Forest\"\n",
    "\n",
    "        elif row[fdic[\"BCLCS_LEVEL_1\"]] == \"N\" and row[fdic[\"BCLCS_LEVEL_2\"]] == \"L\" and (row[fdic[\"HARVEST_DATE\"]] is None or row[fdic[\"HARVEST_DATE\"]] < 1):\n",
    "            row[fdic[\"contclass\"]] = \"X\" \n",
    "            row[fdic[\"THLB_Area\"]] = 0 \n",
    "            row[fdic[\"rollup\"]] = \"2_Non_Forest\"\n",
    "            if row[fdic[\"BCLCS_LEVEL_3\"]] in [\"A\", \"U\"]:\n",
    "                row[fdic[\"netdown\"]] = \"2_07_Non_vegetated_land\" \n",
    "            else:\n",
    "                row[fdic[\"netdown\"]] = \"2_02_Wetlands\"\n",
    "\n",
    "        elif row[fdic[\"BCLCS_LEVEL_1\"]] ==\"N\" and row[fdic[\"BCLCS_LEVEL_2\"]] ==\"W\": \n",
    "            row[fdic[\"contclass\"]] = \"X\"\n",
    "            row[fdic[\"THLB_Area\"]] = 0 \n",
    "            row[fdic[\"rollup\"]] = \"2_Non_Forest\"\n",
    "            if row[fdic[\"BCLCS_LEVEL_3\"]] ==\"A\" and row[fdic[\"BCLCS_LEVEL_5\"]] ==\"LA\": \n",
    "                row[fdic[\"netdown\"]] = \"2_01_Lakes\"\n",
    "            else:\n",
    "                row[fdic[\"netdown\"]] = \"2_03_Rivers\"\n",
    "            if row[fdic[\"BCLCS_LEVEL_3\"]] ==\"W\" and row[fdic[\"BCLCS_LEVEL_5\"]] ==\"OC\": \n",
    "                row[fdic[\"netdown\"]] = \"2_06_Ocean\"\n",
    "            else:\n",
    "                row[fdic[\"netdown\"]] = \"2_02_Wetlands\"\n",
    "        \n",
    "        if row[fdic[\"BCLCS_LEVEL_1\"]] == \"V\" and row[fdic[\"BCLCS_LEVEL_2\"]] == \"N\" and (row[fdic[\"HARVEST_DATE\"]] is None or row[fdic[\"HARVEST_DATE\"]] < 1):\n",
    "            row[fdic[\"contclass\"]] = \"X\" \n",
    "            row[fdic[\"THLB_Area\"]] = 0 \n",
    "            row[fdic[\"rollup\"]] = \"2_Non_Forest\"\n",
    "            if row[fdic[\"BCLCS_LEVEL_3\"]] in [\"A\", \"U\"]: \n",
    "                row[fdic[\"netdown\"]] = \"2_05_Vegetated_Not_Treed\"\n",
    "            else:\n",
    "                row[fdic[\"netdown\"]] = \"2_02_Wetlands\"\n",
    "        \n",
    "        elif row[fdic[\"BCLCS_LEVEL_1\"]] == \"V\" and row[fdic[\"BCLCS_LEVEL_2\"]] == \"T\": \n",
    "            if row[fdic[\"BCLCS_LEVEL_3\"]] == \"W\":\n",
    "                row[fdic[\"contclass\"]] = \"X\" \n",
    "                row[fdic[\"THLB_Area\"]] = 0 \n",
    "                row[fdic[\"netdown\"]] = \"2_02_Wetlands\" \n",
    "                row[fdic[\"rollup\"]] = \"2_Non_Forest\"\n",
    "\n",
    "        elif row[fdic[\"BCLCS_LEVEL_3\"]] == \"U\" and row[fdic[\"SPECIES_CD_1\"]] in [None, \"\"] and (row[fdic[\"HARVEST_DATE\"]] is None or row[fdic[\"HARVEST_DATE\"]] < 1):\n",
    "            row[fdic[\"contclass\"]] = \"X\" \n",
    "            row[fdic[\"THLB_Area\"]] = 0 \n",
    "            row[fdic[\"netdown\"]] = \"2_08_Not_Typed\" \n",
    "            row[fdic[\"rollup\"]] = \"2_Non_Forest\"\n",
    "            \n",
    "        #Roads\n",
    "        elif row[fdic[\"FID_Roads_proj_clip_buff\"]] >0 : \n",
    "            row[fdic[\"contclass\"]] = \"X\" \n",
    "            row[fdic[\"THLB_Area\"]] = 0 \n",
    "            row[fdic[\"netdown\"]] = \"2_04_Roads\" \n",
    "            row[fdic[\"rollup\"]] = \"2_Non_Forest\"\n",
    "            \n",
    "        #if row[fdic[\"FID_F_Roads_clip_buff\"]] >0 : \n",
    "            #row[fdic[\"contclass\"]] = \"X\" \n",
    "            #row[fdic[\"THLB_Area\"]] = 0 \n",
    "            #row[fdic[\"netdown\"]] = \"2_04_Roads\" \n",
    "            #row[fdic[\"rollup\"]] = \"2_Non_Forest\"\n",
    "            \n",
    "        #Water\n",
    "        elif row[fdic[\"FID_Wetlands_proj_clip_buff\"]]>0 : \n",
    "            row[fdic[\"contclass\"]] = \"X\" \n",
    "            row[fdic[\"THLB_Area\"]] = 0 \n",
    "            row[fdic[\"netdown\"]] = \"2_02_Wetlands\" \n",
    "            row[fdic[\"rollup\"]] = \"2_Non_Forest\"\n",
    "\n",
    "        elif row[fdic[\"FID_Rivers_proj_clip\"]]>0 : \n",
    "            row[fdic[\"contclass\"]] = \"X\" \n",
    "            row[fdic[\"THLB_Area\"]] = 0 \n",
    "            row[fdic[\"netdown\"]] = \"2_03_Rivers\" \n",
    "            row[fdic[\"rollup\"]] = \"2_Non_Forest\"\n",
    "\n",
    "        elif row[fdic[\"FID_Lakes_proj_clip\"]]>0 : \n",
    "            row[fdic[\"contclass\"]] = \"X\"\n",
    "            row[fdic[\"THLB_Area\"]] = 0\n",
    "            row[fdic[\"netdown\"]] = \"2_01_Lakes\" \n",
    "            row[fdic[\"rollup\"]] = \"2_Non_Forest\"\n",
    "            \n",
    "         ## Exclusions\n",
    "        elif row[fdic[\"FID_Parks_Prv_proj_clip\"]] >0:\n",
    "            row[fdic[\"contclass\"]] = \"X\" \n",
    "            row[fdic[\"THLB_Area\"]] = 0\n",
    "            row[fdic[\"netdown\"]] = (\"1_03_Park_\"+ str(row[fdic[\"REGION_NAME\"]]))[0:50] \n",
    "            row[fdic[\"rollup\"]] = \"1_Exclusions\"\n",
    "\n",
    "        #if row[fdic[\"FID_Parks_Fed_proj_clip\"]] >0:\n",
    "        #    row[fdic[\"contclass\"]] = \"X\" \n",
    "        #    row[fdic[\"THLB_Area\"]] = 0\n",
    "        #    row[fdic[\"netdown\"]] = (\"1_03_Park_\"+ str(row[fdic[\"ENGLISH_NAME_1\"]]))[0:50] \n",
    "        #    row[fdic[\"rollup\"]] = \"1_Exclusions\"\n",
    "    \n",
    "        elif row[fdic[\"FID_IR_proj_clip\"]] is not None and row[fdic[\"FID_IR_proj_clip\"]] > 0: \n",
    "            row[fdic[\"contclass\"]] = \"X\" \n",
    "            row[fdic[\"THLB_Area\"]] = 0 \n",
    "            row[fdic[\"netdown\"]] = \"1_02_AgricRsv\" \n",
    "            row[fdic[\"rollup\"]] = \"1_Exclusions\"\n",
    "    \n",
    "        elif row[fdic[\"FID_IR_proj_clip\"]]  is not None and row[fdic[\"FID_IR_proj_clip\"]] > 0:\n",
    "            row[fdic[\"contclass\"]] = \"X\" \n",
    "            row[fdic[\"THLB_Area\"]] = 0\n",
    "            row[fdic[\"netdown\"]] = (\"1_01_IR_\"+ str(row[fdic[\"ENGLISH_NAME\"]]))[0:50] \n",
    "            row[fdic[\"rollup\"]] = \"1_Exclusions\"\n",
    "\n",
    "        cursor.updateRow(row)\n",
    "print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b654c9-7639-49eb-9fbf-aef8b6643677",
   "metadata": {},
   "source": [
    "# Add starting age column\n",
    "This script adds fields and updates the stand age to year 2023. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31169a9f-a88d-4205-8838-b7deea165354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcview\n",
    "import arcpy\n",
    "import time\n",
    "Start = time.time()\n",
    "\n",
    "arcpy.env.workspace = r\"E:\\Newmont Carbon Project\\Data\\Data BC\\00_Script_creation_Test\\Resultant_v1.gdb\"\n",
    "\n",
    "fc = \"Resultant_v1\"\n",
    "\n",
    "try:\n",
    "    arcpy.AddField_management(fc, \"Age_2023\", \"LONG\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "## short code to enable the use of field names\n",
    "flist = arcpy.ListFields(fc)\n",
    "fdic = {}\n",
    "fl = []\n",
    "for f in flist:\n",
    "    fdic[f.name] = flist.index(f)\n",
    "    fl.append(f.name)\n",
    "    \n",
    "#the update cursor\n",
    "with arcpy.da.UpdateCursor(fc, fl) as cursor:\n",
    "    for row in cursor:\n",
    "        row[fdic[\"Age_2023\"]]=None\n",
    "        #if row[fdic[\"FID_Results\"]]>0:\n",
    "            #row[fdic[\"Age_2023\"]]=2015-row[fdic[\"Dist_Year\"]]\n",
    "        if row[fdic[\"PROJ_AGE_1\"]] != None:\n",
    "            row[fdic[\"Age_2023\"]] = row[fdic[\"PROJ_AGE_1\"]]+2\n",
    "        elif row[fdic[\"HARVEST_DATE\"]]!= None:\n",
    "            row[fdic[\"Age_2023\"]]=2015-int(str(row[fdic[\"HARVEST_DATE\"]])[:4])\n",
    "        cursor.updateRow(row)\n",
    "print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c33761-5760-46b3-b5ba-5e66e64e4518",
   "metadata": {},
   "source": [
    "# Add AU fields\n",
    "Adds AU filds and updates the AU based on the AU definitions from the Cassiar TSA.\n",
    "The coastal/transitional groupings are the CWH, ESSF,\n",
    "ICH, SBS, and MH zones, while the interior groupings are the BWBS and SWB zones.\n",
    "\n",
    "Leading species present in area: AT, BL, HM, PL, SX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ce1359-09a2-49d6-8683-4e0280376a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import time\n",
    "\n",
    "Start = time.time()\n",
    "\n",
    "arcpy.env.workspace = r\"E:\\Newmont Carbon Project\\Data\\Data BC\\00_Script_creation_Test\\Resultant_v1.gdb\"\n",
    "\n",
    "fc = \"Resultant_v1\"\n",
    "\n",
    "try:\n",
    "    arcpy.AddField_management(fc, \"AU\", \"LONG\")\n",
    "except arcpy.ExecuteError:\n",
    "    pass\n",
    "\n",
    "# Short code to enable the use of field names\n",
    "flist = arcpy.ListFields(fc)\n",
    "fdic = {f.name: flist.index(f) for f in flist}\n",
    "fl = list(fdic.keys())\n",
    "\n",
    "# Define species lists\n",
    "Aspen = ['AC', 'ACT', 'AT', 'EP', 'VB', 'MB']\n",
    "Bal = ['B', 'BA', 'BG', 'BL']\n",
    "Cedar = ['CW', 'YC']\n",
    "Alder = ['D', 'DR']\n",
    "DougFir = ['F', 'FD', 'FDC', 'FDI']\n",
    "Hem = ['H', 'HM', 'HW']\n",
    "Pine = ['PA', 'PL', 'PLC', 'PW', 'PLI', 'PY']\n",
    "Spruce = ['S', 'SS', 'SW', 'SX', 'SE', 'SXW', 'SB']\n",
    "\n",
    "# The update cursor\n",
    "with arcpy.da.UpdateCursor(fc, fl) as cursor:\n",
    "    for row in cursor:\n",
    "        \n",
    "        # Stands\n",
    "        species_cd_1 = row[fdic[\"SPECIES_CD_1\"]]\n",
    "        site_index = row[fdic[\"SITE_INDEX\"]]\n",
    "        bec_zone_code = row[fdic[\"BEC_ZONE_CODE\"]]\n",
    "        herd_name = row[fdic[\"HERD_NAME\"]] \n",
    "\n",
    "        # Put Decid into its own group\n",
    "        if species_cd_1 in Aspen:\n",
    "            row[fdic[\"AU\"]] = 33\n",
    "\n",
    "        # Coastal-Spruce\n",
    "        elif species_cd_1 in Spruce and bec_zone_code in [\"CWH\", \"ESSF\", \"ICH\", \"SBS\", \"MH\"]:\n",
    "            if site_index is None or 17.1 <= site_index <= 25.0:\n",
    "                row[fdic[\"AU\"]] = 2\n",
    "            elif site_index >= 25.1:\n",
    "                row[fdic[\"AU\"]] = 1\n",
    "            elif 0.1 < site_index < 17.1:\n",
    "                row[fdic[\"AU\"]] = 3\n",
    "\n",
    "        # Interior-Spruce\n",
    "        elif species_cd_1 in Spruce and bec_zone_code in [\"BWBS\", \"SWB\"]:\n",
    "            if site_index is None or 17.1 <= site_index <= 25.0:\n",
    "                row[fdic[\"AU\"]] = 5\n",
    "            elif site_index >= 25.1:\n",
    "                row[fdic[\"AU\"]] = 4\n",
    "            elif 0.1 < site_index < 17.1:\n",
    "                row[fdic[\"AU\"]] = 6\n",
    "\n",
    "        # Alpine Spruce\n",
    "        elif species_cd_1 in Spruce and bec_zone_code in [\"BAFA\", \"CMA\"]:\n",
    "            row[fdic[\"AU\"]] = 7\n",
    "\n",
    "        # Coastal Pine\n",
    "        elif species_cd_1 in Pine and bec_zone_code in [\"CWH\", \"ESSF\", \"ICH\", \"SBS\", \"MH\"]:\n",
    "            if site_index is None or 14.6 <= site_index <= 19.0:\n",
    "                row[fdic[\"AU\"]] = 9\n",
    "            elif site_index >= 19.1:\n",
    "                row[fdic[\"AU\"]] = 8\n",
    "            elif 0.1 < site_index < 14.6:\n",
    "                row[fdic[\"AU\"]] = 10\n",
    "\n",
    "        # Interior Pine\n",
    "        elif species_cd_1 in Pine and bec_zone_code in [\"BWBS\", \"SWB\"]:\n",
    "            if site_index is None or 14.6 <= site_index <= 19.0:\n",
    "                row[fdic[\"AU\"]] = 12\n",
    "            elif site_index >= 19.1:\n",
    "                row[fdic[\"AU\"]] = 11\n",
    "            elif 0.1 < site_index < 14.6:\n",
    "                row[fdic[\"AU\"]] = 13\n",
    "\n",
    "        # Alpine Pine\n",
    "        elif species_cd_1 in Pine and bec_zone_code in [\"BAFA\", \"CMA\"]:\n",
    "            if site_index is None or 14.6 <= site_index <= 19.0:\n",
    "                row[fdic[\"AU\"]] = 15\n",
    "            elif site_index >= 19.1:\n",
    "                row[fdic[\"AU\"]] = 14\n",
    "            elif 0.1 < site_index < 14.6:\n",
    "                row[fdic[\"AU\"]] = 16\n",
    "\n",
    "   # Coastal Balsam\n",
    "        if species_cd_1 in Bal and bec_zone_code in [\"CWH\", \"ESSF\", \"ICH\", \"SBS\", \"MH\"]:\n",
    "            if site_index is None or 13.1 <= site_index <= 16.0:\n",
    "                row[fdic[\"AU\"]] = 18\n",
    "            elif site_index >= 16.1:\n",
    "                row[fdic[\"AU\"]] = 17\n",
    "            elif 0.1 < site_index < 13.1:\n",
    "                row[fdic[\"AU\"]] = 19\n",
    "\n",
    "         # Interior Balsam\n",
    "        elif species_cd_1 in Bal and bec_zone_code in [\"BWBS\", \"SWB\"]:\n",
    "            if site_index is None or 13.1 <= site_index <= 16.0:\n",
    "                row[fdic[\"AU\"]] = 21\n",
    "            elif site_index >= 16.1:\n",
    "                row[fdic[\"AU\"]] = 20\n",
    "            elif 0.1 < site_index < 13.1:\n",
    "                row[fdic[\"AU\"]] = 22\n",
    "                \n",
    "        # Alpine Balsam\n",
    "        elif species_cd_1 in Bal and bec_zone_code in [\"BAFA\", \"CMA\"]:\n",
    "            row[fdic[\"AU\"]] = 23\n",
    "\n",
    "   # Coastal Hemlock\n",
    "        if species_cd_1 in Hem and bec_zone_code in [\"CWH\", \"ESSF\", \"ICH\", \"SBS\", \"MH\"]:\n",
    "            if site_index is None or 14.1 <= site_index <= 17.0:\n",
    "                row[fdic[\"AU\"]] = 25\n",
    "            elif site_index >= 17.1:\n",
    "                row[fdic[\"AU\"]] = 24\n",
    "            elif 0.1 < site_index < 14.1:\n",
    "                row[fdic[\"AU\"]] = 26\n",
    "\n",
    "         # Interior Hemlock\n",
    "        elif species_cd_1 in Hem and bec_zone_code in [\"BWBS\", \"SWB\"]:\n",
    "            if site_index is None or 14.1 <= site_index <= 17.0:\n",
    "                row[fdic[\"AU\"]] = 28\n",
    "            elif site_index >= 17.1:\n",
    "                row[fdic[\"AU\"]] = 27\n",
    "            elif 0.1 < site_index < 14.1:\n",
    "                row[fdic[\"AU\"]] = 29\n",
    "                \n",
    "        # Alpine Hemlock\n",
    "        elif species_cd_1 in Hem and bec_zone_code in [\"BAFA\", \"CMA\"]:\n",
    "            if site_index is None or 14.1 <= site_index <= 17.0:\n",
    "                row[fdic[\"AU\"]] = 30\n",
    "            elif 0.1 < site_index < 14.1:\n",
    "                row[fdic[\"AU\"]] = 31\n",
    "\n",
    "#Caribou stands (overwrites the previous stands if these stands are in the Caribou zone)\n",
    "        if herd_name in [\"Level-Kawdy\"]:\n",
    "            row[fdic[\"AU\"]]=32\n",
    "\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")\n",
    "\n",
    " \n",
    "                \n",
    "# #Caribou stands (overwrites the previous stands if these stands are in the Caribou zone)\n",
    "#         if row[fdic[\"HERD_NAME\"]]== \"Itcha-Ilgachuz\":\n",
    "#             if row[fdic[\"FID_Results\"]]>0:\n",
    "#                 row[fdic[\"AU\"]]=7\n",
    "#             else:\n",
    "#                 row[fdic[\"AU\"]]=6\n",
    "\n",
    "# #MDWR stands (overwrites the previous non-MPB and Caribou stands if in MDWR zone)\n",
    "#         if row[fdic[\"FID_UWR_proj_clip\"]]>1:\n",
    "#             if row[fdic[\"SPECIES_CD_1\"]] in DougFir and row[fdic[\"SPECIES_PCT_1\"]] >=40:\n",
    "#                 if row[fdic[\"BEC_ZONE_CODE\"]] in [\"IDF\", \"SBPS\"]:\n",
    "#                     if row[fdic[\"HARVEST_DATE\"]] not in ['',None] or row[fdic[\"OPENING_IND\"]]== \"Y\" or row[fdic[\"FID_Results\"]]>0:\n",
    "#                         row[fdic[\"AU\"]]=2\n",
    "#                     else:\n",
    "#                         row[fdic[\"AU\"]]=3\n",
    "#                 if row[fdic[\"BEC_ZONE_CODE\"]] in [\"SBS\", \"ICH\"]:\n",
    "#                     row[fdic[\"AU\"]]=4\n",
    "#                 else:\n",
    "#                     row[fdic[\"AU\"]]=1\n",
    "#             else:\n",
    "#                 row[fdic[\"AU\"]]=5\n",
    "\n",
    "# #recent harvest blocks with no species info assumed PL_med\n",
    "# if row[fdic[\"AU\"]]==None and (row[fdic[\"HARVEST_DATE\"]] not in ['',None] or row[fdic[\"FID_Results\"]]>0):\n",
    "# row[fdic[\"AU\"]]=17\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea912a7-3e57-4b70-9a12-faf957c36a01",
   "metadata": {},
   "source": [
    "# Resultant Create Unique ID\n",
    "This section of code add a field and populates it with a unique integer and functions to make sure that each block has a unique ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ef77e8-e154-4c22-8d47-23f8a44d71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import time\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "Start = time.time()\n",
    "\n",
    "# Defining the workspace\n",
    "arcpy.env.workspace = r\"E:\\Newmont Carbon Project\\Data\\Data BC\\00_Script_creation_Test\\Resultant_v1.gdb\"\n",
    "fc = \"Resultant_v1\"\n",
    "\n",
    "# Adding the field\n",
    "try:\n",
    "    arcpy.AddField_management(fc, \"Block_ID\", \"LONG\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create the function for auto-increment\n",
    "def get_last_incremented_value():\n",
    "    try:\n",
    "        with open(get_increment_file_path(), \"r\") as file:\n",
    "            return int(file.readline().strip())\n",
    "    except FileNotFoundError:\n",
    "        return 0\n",
    "\n",
    "def update_last_incremented_value(value):\n",
    "    with open(get_increment_file_path(), \"w\") as file:\n",
    "        file.write(str(value))\n",
    "\n",
    "def get_increment_file_path():\n",
    "    return os.path.join(tempfile.gettempdir(), \"last_incremented_value.txt\")\n",
    "\n",
    "# Get the last incremented value\n",
    "rec = get_last_incremented_value()\n",
    "\n",
    "# Create update cursor for feature class\n",
    "with arcpy.da.UpdateCursor(fc, [\"Block_ID\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        rec += 1\n",
    "        row[0] = rec\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "# Update the last incremented value\n",
    "update_last_incremented_value(rec)\n",
    "\n",
    "print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2c80fb-35df-4eca-8358-87b21d7adb3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sum the NetDown to put into table\n",
    "## Rollup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e89a03-6fc7-4db6-bb71-bdeadca6a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import arcpy\n",
    "import time\n",
    "\n",
    "Start = time.time()\n",
    "\n",
    "# Set the workspace location:\n",
    "arcpy.env.workspace = r\"E:\\Newmont Carbon Project\\Data\\Data BC\\00_Script_creation_Test\\Resultant_v1.gdb\"\n",
    "\n",
    "# Identify the feature class (table to work on)\n",
    "fc = \"Resultant_v1\"\n",
    "\n",
    "# This bit of code enables you to reference the field name rather than referencing the row in the list by the index position (i.e. row[0], row[1])\n",
    "flist = arcpy.ListFields(fc)\n",
    "fdic = {}\n",
    "fl = []\n",
    "for f in flist:\n",
    "    fdic[f.name] = flist.index(f)\n",
    "    fl.append(f.name)\n",
    "\n",
    "# This declares the dictionary that will be used to summarize the data\n",
    "sumdict = {}\n",
    "\n",
    "# Using the data access search cursor, go through all the rows. If the dictionary has a new key, add it to the list and add the area. Otherwise, if one is already added, just keep adding to the key\n",
    "with arcpy.da.SearchCursor(fc, fl) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[fdic[\"rollup\"]] in sumdict:\n",
    "            sumdict[row[fdic[\"rollup\"]]] += row[fdic[\"Shape_Area\"]] / 10000\n",
    "        else:\n",
    "            sumdict[row[fdic[\"rollup\"]]] = row[fdic[\"Shape_Area\"]] / 10000\n",
    "\n",
    "# This section makes and sorts a list of the items in the sumdict dictionary and appends them into a CSV-like table\n",
    "l = list(sumdict.items())\n",
    "l.sort()\n",
    "d = [[\"rollup\", \"Hectares\"]]\n",
    "for r in l:\n",
    "    d.append(r)\n",
    "\n",
    "\n",
    "# Specifies the output directory and filename of the CSV file\n",
    "output_directory = r\"E:\\Newmont Carbon Project\\Data\\Data BC\\00_Script_creation_Test\\01_Resultant_Stats\"\n",
    "output_filename = \"SUM_rollup.csv\"\n",
    "output_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open(output_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerows(d)\n",
    "\n",
    "print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c62f61b-4a05-4f12-849b-06f532cdb72a",
   "metadata": {},
   "source": [
    "## Netdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf1789d-50f6-4725-b7c8-e0fa62e04051",
   "metadata": {},
   "outputs": [],
   "source": [
    "## summarize net down\n",
    "\n",
    "import csv\n",
    "import arcpy\n",
    "import time\n",
    "\n",
    "Start = time.time()\n",
    "\n",
    "# Set the workspace location:\n",
    "arcpy.env.workspace = r\"E:\\Newmont Carbon Project\\Data\\Data BC\\00_Script_creation_Test\\Resultant_v1.gdb\"\n",
    "\n",
    "# Identify the feature class (table to work on)\n",
    "fc = \"Resultant_v1\"\n",
    "\n",
    "# This bit of code enables you to reference the field name rather than referencing the row in the list by the index position (i.e. row[0], row[1])\n",
    "flist = arcpy.ListFields(fc)\n",
    "fdic = {}\n",
    "fl = []\n",
    "for f in flist:\n",
    "    fdic[f.name] = flist.index(f)\n",
    "    fl.append(f.name)\n",
    "\n",
    "# This declares the dictionary that will be used to summarize the data\n",
    "sumdict = {}\n",
    "\n",
    "# Using the data access search cursor, go through all the rows. If the dictionary has a new key, add it to the list and add the area. Otherwise, if one is already added, just keep adding to the key\n",
    "with arcpy.da.SearchCursor(fc, fl) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[fdic[\"netdown\"]] in sumdict:\n",
    "            sumdict[row[fdic[\"netdown\"]]]=sumdict[row[fdic[\"netdown\"]]]+row[fdic[\"Shape_Area\"]]/10000\n",
    "        else:\n",
    "            sumdict[row[fdic[\"netdown\"]]] = row[fdic[\"Shape_Area\"]] / 10000\n",
    "\n",
    "##This section makes and sorts a list of the items in the sumdict dictionary and appends them into a CSV like table\n",
    "l = sumdict.items()\n",
    "l = sorted(l)\n",
    "d = [[\"netdown\", \"Hectares\"]]\n",
    "for r in l:\n",
    "    d.append(r)\n",
    "\n",
    "            \n",
    "# Specifies the output directory and filename of the CSV file\n",
    "output_directory = r\"E:\\Newmont Carbon Project\\Data\\Data BC\\00_Script_creation_Test\\01_Resultant_Stats\"\n",
    "output_filename = \"SUM_netdown.csv\"\n",
    "output_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open(output_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerows(d)\n",
    "\n",
    "print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827f34ba-526d-4a60-9680-f9fde3d589b1",
   "metadata": {},
   "source": [
    "## Analysis Units\n",
    "Summarizes the AU's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d8183e-ce5c-47fb-9395-85842b10ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import arcpy\n",
    "import time\n",
    "import os\n",
    "\n",
    "Start = time.time()\n",
    "\n",
    "# Set the workspace location:\n",
    "arcpy.env.workspace = r\"E:\\Newmont Carbon Project\\Data\\Data BC\\00_Script_creation_Test\\Resultant_v1.gdb\"\n",
    "\n",
    "# Identify the feature class (table to work on)\n",
    "fc = \"Resultant_v1\"\n",
    "\n",
    "# This bit of code enables you to reference the field name rather than referencing the row in the list by the index position (i.e. row[0], row[1])\n",
    "flist = arcpy.ListFields(fc)\n",
    "fdic = {}\n",
    "fl = []\n",
    "for f in flist:\n",
    "    fdic[f.name] = flist.index(f)\n",
    "    fl.append(f.name)\n",
    "\n",
    "# This declares the dictionary that will be used to summarize the data\n",
    "sumdict = {}\n",
    "\n",
    "# Using the data access search cursor, go through all the rows; if the dictionary has a new key, add it to the list and add the area; otherwise, if one is already added, just keep adding to the key\n",
    "with arcpy.da.SearchCursor(fc, fl) as cursor:\n",
    "    for row in cursor:\n",
    "        shape_area = row[fdic[\"Shape_Area\"]]\n",
    "        au_value = row[fdic[\"AU\"]]\n",
    "        if shape_area is not None and au_value is not None:  # Filter out records with None values\n",
    "            if au_value in sumdict:\n",
    "                sumdict[au_value] += shape_area / 10000\n",
    "            else:\n",
    "                sumdict[au_value] = shape_area / 10000\n",
    "\n",
    "# This section makes and sorts a list of the items in the sumdict dictionary and appends them into a CSV-like table\n",
    "l = sorted(sumdict.items())\n",
    "header = [[\"AU\", \"Hectares\"]]\n",
    "data = header + l\n",
    "\n",
    "# Specifies the output directory and filename of the CSV file\n",
    "output_directory = r\"E:\\Newmont Carbon Project\\Data\\Data BC\\00_Script_creation_Test\\01_Resultant_Stats\"\n",
    "output_filename = \"SUM_analysisunits.csv\"\n",
    "output_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open(output_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerows(data)\n",
    "\n",
    "print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
